import { createContext, useContext, useState, useCallback, useEffect, useRef } from "react";
import { useLocation } from "wouter";
import { useResolvedLanguage } from "@/hooks/useResolvedLanguage";

type VoiceCommand = {
  command: string;
  action: () => void;
  description: string;
};

type VoiceAssistantProviderProps = {
  children: React.ReactNode;
};

type VoiceAssistantProviderState = {
  isListening: boolean;
  isSpeaking: boolean;
  isSupported: boolean;
  startListening: () => void;
  stopListening: () => void;
  speak: (text: string, lang?: string) => Promise<void>;
  stopSpeaking: () => void;
  registerCommand: (command: VoiceCommand) => void;
  unregisterCommand: (command: string) => void;
};

const VoiceAssistantContext = createContext<VoiceAssistantProviderState | undefined>(
  undefined
);

// Check if browser supports Web Speech API
const isSpeechRecognitionSupported = () => {
  return 'SpeechRecognition' in window || 'webkitSpeechRecognition' in window;
};

const isSpeechSynthesisSupported = () => {
  return 'speechSynthesis' in window;
};

export function VoiceAssistantProvider({ children }: VoiceAssistantProviderProps) {
  const [location] = useLocation();
  const [isListening, setIsListening] = useState(false);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [isSupported] = useState(
    isSpeechRecognitionSupported() && isSpeechSynthesisSupported()
  );
  
  const recognitionRef = useRef<any>(null);
  const commandsRef = useRef<Map<string, VoiceCommand>>(new Map());

  // Use unified route-aware language detection hook
  const currentLang = useResolvedLanguage();

  // Track voice command events
  const trackEvent = useCallback(async (
    eventType: string,
    eventAction: string,
    eventValue?: string
  ) => {
    if (typeof window === 'undefined') return;
    
    try {
      await fetch('/api/accessibility/track', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          eventType,
          eventAction,
          eventValue,
          language: currentLang,
          pageUrl: location,
        }),
      });
    } catch (error) {
      console.debug('Accessibility tracking failed:', error);
    }
  }, [currentLang, location]);

  // Initialize Speech Recognition with language support
  useEffect(() => {
    if (!isSpeechRecognitionSupported()) return;

    const SpeechRecognition = 
      (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
    
    const recognition = new SpeechRecognition();
    
    // Set recognition language based on current language (including route-based Urdu)
    if (currentLang === 'ar') {
      recognition.lang = 'ar-SA'; // Arabic (Saudi Arabia)
    } else if (currentLang === 'ur') {
      recognition.lang = 'ur-PK'; // Urdu (Pakistan)  
    } else {
      recognition.lang = 'en-US'; // English (United States)
    }
    
    recognition.continuous = false;
    recognition.interimResults = false;

    recognition.onstart = () => {
      setIsListening(true);
      // Dispatch event for UI updates
      window.dispatchEvent(new CustomEvent('voice:listening-start'));
    };

    recognition.onend = () => {
      setIsListening(false);
      window.dispatchEvent(new CustomEvent('voice:listening-end'));
    };

    recognition.onresult = (event: any) => {
      const transcript = event.results[0][0].transcript.toLowerCase().trim();
      
      // Dispatch event with transcript
      window.dispatchEvent(
        new CustomEvent('voice:command', {
          detail: { transcript, confidence: event.results[0][0].confidence },
        })
      );

      // Track voice command
      trackEvent('voiceCommand', 'executed', transcript);

      // Check for registered commands
      commandsRef.current.forEach((cmd) => {
        if (transcript.includes(cmd.command.toLowerCase())) {
          cmd.action();
        }
      });
    };

    recognition.onerror = (event: any) => {
      // Graceful degradation for common errors
      const errorType = event.error;
      
      // Don't show user-facing errors for technical issues
      if (errorType === 'no-speech' || errorType === 'audio-capture') {
        console.warn('Speech API unavailable:', errorType);
        setIsListening(false);
        return;
      }
      
      // Only log and dispatch critical errors
      if (errorType === 'not-allowed' || errorType === 'service-not-allowed') {
        console.error('Speech recognition permission denied:', errorType);
        window.dispatchEvent(
          new CustomEvent('voice:error', {
            detail: { 
              error: errorType
              // userMessage will be generated by VoiceAssistantButton based on language
            },
          })
        );
      } else {
        console.warn('Speech recognition error:', errorType);
      }
      
      setIsListening(false);
    };

    recognitionRef.current = recognition;

    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.stop();
      }
    };
  }, [currentLang]); // Recreate recognition when language changes (including route-based)

  const startListening = useCallback(() => {
    if (!isSupported || !recognitionRef.current || isListening) return;
    
    // Note: userMessage will be shown by VoiceAssistantButton based on language
    // We no longer send hard-coded Arabic messages here
    
    // Check for HTTPS requirement (required for Speech API in most browsers)
    if (window.location.protocol !== 'https:' && window.location.hostname !== 'localhost') {
      console.warn('Speech Recognition requires HTTPS. Current protocol:', window.location.protocol);
      window.dispatchEvent(
        new CustomEvent('voice:error', {
          detail: { 
            error: 'https-required'
            // userMessage will be generated by VoiceAssistantButton based on language
          },
        })
      );
      return;
    }
    
    try {
      recognitionRef.current.start();
    } catch (error: any) {
      // Handle "already started" error gracefully
      if (error.name === 'InvalidStateError') {
        console.warn('Speech recognition already in progress');
        return;
      }
      console.error('Failed to start speech recognition:', error);
      window.dispatchEvent(
        new CustomEvent('voice:error', {
          detail: { 
            error: error.name || 'unknown'
            // userMessage will be generated by VoiceAssistantButton based on language
          },
        })
      );
    }
  }, [isSupported, isListening]);

  const stopListening = useCallback(() => {
    if (!recognitionRef.current || !isListening) return;
    
    try {
      recognitionRef.current.stop();
    } catch (error) {
      console.error('Failed to stop speech recognition:', error);
    }
  }, [isListening]);

  const speak = useCallback(
    async (text: string, lang: string = 'ar-SA'): Promise<void> => {
      if (!isSpeechSynthesisSupported()) {
        console.warn('Speech synthesis not supported');
        return;
      }

      // Stop any ongoing speech
      window.speechSynthesis.cancel();

      return new Promise((resolve, reject) => {
        const utterance = new SpeechSynthesisUtterance(text);
        utterance.lang = lang;
        utterance.rate = 1.0;
        utterance.pitch = 1.0;
        utterance.volume = 1.0;

        utterance.onstart = () => {
          setIsSpeaking(true);
          window.dispatchEvent(
            new CustomEvent('voice:speaking-start', { detail: { text } })
          );
        };

        utterance.onend = () => {
          setIsSpeaking(false);
          window.dispatchEvent(
            new CustomEvent('voice:speaking-end', { detail: { text } })
          );
          resolve();
        };

        utterance.onerror = (event) => {
          setIsSpeaking(false);
          window.dispatchEvent(
            new CustomEvent('voice:speaking-error', {
              detail: { error: event.error },
            })
          );
          reject(event);
        };

        window.speechSynthesis.speak(utterance);
      });
    },
    []
  );

  const stopSpeaking = useCallback(() => {
    if (!isSpeechSynthesisSupported()) return;
    window.speechSynthesis.cancel();
    setIsSpeaking(false);
  }, []);

  const registerCommand = useCallback((command: VoiceCommand) => {
    commandsRef.current.set(command.command.toLowerCase(), command);
  }, []);

  const unregisterCommand = useCallback((command: string) => {
    commandsRef.current.delete(command.toLowerCase());
  }, []);

  return (
    <VoiceAssistantContext.Provider
      value={{
        isListening,
        isSpeaking,
        isSupported,
        startListening,
        stopListening,
        speak,
        stopSpeaking,
        registerCommand,
        unregisterCommand,
      }}
    >
      {children}
    </VoiceAssistantContext.Provider>
  );
}

export const useVoiceAssistant = () => {
  const context = useContext(VoiceAssistantContext);
  if (!context) {
    throw new Error("useVoiceAssistant must be used within VoiceAssistantProvider");
  }
  return context;
};

// Utility hook to register voice commands
export const useVoiceCommand = (
  command: string,
  action: () => void,
  description: string
) => {
  const { registerCommand, unregisterCommand } = useVoiceAssistant();

  useEffect(() => {
    registerCommand({ command, action, description });
    return () => unregisterCommand(command);
  }, [command, action, description, registerCommand, unregisterCommand]);
};
